{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b8b2a",
   "metadata": {},
   "source": [
    "here is the code of the tiramisu model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "858d183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super().__init__()\n",
    "        self.add_module(\"norm\", nn.BatchNorm2d(in_channels))\n",
    "        self.add_module(\"silu\", nn.SiLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv\",\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "        )\n",
    "        self.add_module(\"drop\", nn.Dropout2d(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.layers = nn.ModuleList([DenseLayer(in_channels + i * growth_rate, growth_rate) for i in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            new_features = []\n",
    "            # we pass all previous activations into each dense layer normally\n",
    "            # But we only store each dense layer's output in the new_features array\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)\n",
    "                new_features.append(out)\n",
    "            return torch.cat(new_features, 1)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)  # 1 = channel axis\n",
    "            return x\n",
    "\n",
    "\n",
    "class TransitionDown(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.add_module(\"norm\", nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module(\"SiLU\", nn.SiLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv\",\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.add_module(\"drop\", nn.Dropout2d(0.2))\n",
    "        self.add_module(\"maxpool\", nn.MaxPool2d(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convTrans = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        out = self.convTrans(x)\n",
    "        out = center_crop(out, skip.size(2), skip.size(3))\n",
    "        out = torch.cat([out, skip], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers):\n",
    "        super().__init__()\n",
    "        self.add_module(\"bottleneck\", DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "def center_crop(layer, max_height, max_width):\n",
    "    _, _, h, w = layer.size()\n",
    "    xy1 = (w - max_width) // 2\n",
    "    xy2 = (h - max_height) // 2\n",
    "    return layer[:, :, xy2 : (xy2 + max_height), xy1 : (xy1 + max_width)]\n",
    "\n",
    "\n",
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        down_blocks=(4, 4, 4, 4, 4),\n",
    "        up_blocks=(4, 4, 4, 4, 4),\n",
    "        bottleneck_layers=4,\n",
    "        growth_rate=12,\n",
    "        out_chans_first_conv=48,\n",
    "        n_classes=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.down_blocks = down_blocks\n",
    "        self.up_blocks = up_blocks\n",
    "        cur_channels_count = 0\n",
    "        skip_connection_channel_counts = []\n",
    "\n",
    "        ## First Convolution ##\n",
    "\n",
    "        self.add_module(\n",
    "            \"firstconv\",\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_chans_first_conv,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "        cur_channels_count = out_chans_first_conv\n",
    "\n",
    "        #####################\n",
    "        # Downsampling path #\n",
    "        #####################\n",
    "\n",
    "        self.denseBlocksDown = nn.ModuleList([])\n",
    "        self.transDownBlocks = nn.ModuleList([])\n",
    "        for i in range(len(down_blocks)):\n",
    "            self.denseBlocksDown.append(DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n",
    "            cur_channels_count += growth_rate * down_blocks[i]\n",
    "            skip_connection_channel_counts.insert(0, cur_channels_count)\n",
    "            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n",
    "\n",
    "        #####################\n",
    "        #     Bottleneck    #\n",
    "        #####################\n",
    "\n",
    "        self.add_module(\"bottleneck\", Bottleneck(cur_channels_count, growth_rate, bottleneck_layers))\n",
    "        prev_block_channels = growth_rate * bottleneck_layers\n",
    "        cur_channels_count += prev_block_channels\n",
    "\n",
    "        #######################\n",
    "        #   Upsampling path   #\n",
    "        #######################\n",
    "\n",
    "        self.transUpBlocks = nn.ModuleList([])\n",
    "        self.denseBlocksUp = nn.ModuleList([])\n",
    "        for i in range(len(up_blocks) - 1):\n",
    "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
    "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
    "\n",
    "            self.denseBlocksUp.append(DenseBlock(cur_channels_count, growth_rate, up_blocks[i], upsample=True))\n",
    "            prev_block_channels = growth_rate * up_blocks[i]\n",
    "            cur_channels_count += prev_block_channels\n",
    "\n",
    "        ## Final DenseBlock ##\n",
    "\n",
    "        self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
    "        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n",
    "\n",
    "        self.denseBlocksUp.append(DenseBlock(cur_channels_count, growth_rate, up_blocks[-1], upsample=False))\n",
    "        cur_channels_count += growth_rate * up_blocks[-1]\n",
    "\n",
    "        ## Softmax ##\n",
    "\n",
    "        self.finalConv = nn.Conv2d(\n",
    "            in_channels=cur_channels_count,\n",
    "            out_channels=n_classes,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.firstconv(x)\n",
    "\n",
    "        skip_connections = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            out = self.denseBlocksDown[i](out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.transDownBlocks[i](out)\n",
    "            \n",
    "        ################### get the output of the bottleneck layer ##########################\n",
    "        out = self.bottleneck(out)\n",
    "        bottleneck_output = out\n",
    "\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            skip = skip_connections.pop()\n",
    "            out = self.transUpBlocks[i](out, skip)\n",
    "            out = self.denseBlocksUp[i](out)\n",
    "\n",
    "        out = self.finalConv(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, bottleneck_output\n",
    "\n",
    "# the class 0 is the background, 1 is RV, 2 is MYO, 3 is LV\n",
    "tiramisu = FCDenseNet(in_channels=1, n_classes=4) # the class 0 is the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3139da1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCDenseNet(\n",
       "  (firstconv): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (denseBlocksDown): ModuleList(\n",
       "    (0): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(60, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(72, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(84, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(108, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(120, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(132, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(144, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(156, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(168, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(180, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(204, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(216, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(228, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(240, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(252, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(264, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(276, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transDownBlocks): ModuleList(\n",
       "    (0): TransitionDown(\n",
       "      (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (SiLU): SiLU(inplace=True)\n",
       "      (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d(p=0.2, inplace=False)\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): TransitionDown(\n",
       "      (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (SiLU): SiLU(inplace=True)\n",
       "      (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d(p=0.2, inplace=False)\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): TransitionDown(\n",
       "      (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (SiLU): SiLU(inplace=True)\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d(p=0.2, inplace=False)\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): TransitionDown(\n",
       "      (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (SiLU): SiLU(inplace=True)\n",
       "      (conv): Conv2d(240, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d(p=0.2, inplace=False)\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): TransitionDown(\n",
       "      (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (SiLU): SiLU(inplace=True)\n",
       "      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d(p=0.2, inplace=False)\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Bottleneck(\n",
       "    (bottleneck): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(288, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(300, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(312, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(324, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transUpBlocks): ModuleList(\n",
       "    (0-4): 5 x TransitionUp(\n",
       "      (convTrans): ConvTranspose2d(48, 48, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (denseBlocksUp): ModuleList(\n",
       "    (0): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(336, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(348, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(360, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(372, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(288, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(300, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(312, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(324, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(240, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(252, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(264, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(276, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(204, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(216, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(228, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DenseBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): DenseLayer(\n",
       "          (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(144, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): DenseLayer(\n",
       "          (norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(156, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): DenseLayer(\n",
       "          (norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(168, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): DenseLayer(\n",
       "          (norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU(inplace=True)\n",
       "          (conv): Conv2d(180, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (finalConv): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tiramisu_trained_model = torch.load(\"tiramisu_acdc.pt\", weights_only=False)\n",
    "tiramisu_state_dict = tiramisu_trained_model.state_dict()\n",
    "# remove '_orig_mod.\" from the keys of the state_dict\n",
    "tiramisu_state_dict = {k.replace('_orig_mod.', ''): v for k, v in tiramisu_state_dict.items()}\n",
    "tiramisu.load_state_dict(tiramisu_state_dict)\n",
    "tiramisu.eval()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tiramisu = tiramisu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ec0cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 1, 128, 128)\n",
    "output, bottleneck_output = tiramisu(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84de44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(bottleneck_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039f68f",
   "metadata": {},
   "source": [
    "# make prediction for the table2 data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nrrd\n",
    "import glob\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from skimage.transform import resize\n",
    "\n",
    "def crop_resize_image(image, new_dim=256):\n",
    "    \"\"\"\n",
    "    Process a 3D numpy image by removing non-zero background, cropping to square,\n",
    "    resizing, and saving crop_index as slice objects for restoration.\n",
    "    \n",
    "    Parameters:\n",
    "    image (np.ndarray): Input image of shape (x, y, z)\n",
    "    new_dim (int): Desired dimension for the output square image (new_dim, new_dim, z)\n",
    "    Returns:\n",
    "    tuple: (processed_image, restore_info)\n",
    "        - processed_image: Processed image of shape (new_dim, new_dim, z)\n",
    "        - restore_info: Dict containing original_shape, crop_index, original_dim, new_dim\n",
    "    \"\"\"\n",
    "    # Step 1: Remove non-zero background using np.nonzero\n",
    "    nz = np.nonzero(image)\n",
    "    \n",
    "    \n",
    "    # Get min and max indices\n",
    "    min_indices = np.min(nz, axis=1)\n",
    "    max_indices = np.max(nz, axis=1)\n",
    "    \n",
    "    # Create crop index for non-zero region\n",
    "    crop_index = tuple(slice(imin, imax + 1) for imin, imax in zip(min_indices, max_indices))\n",
    "    \n",
    "    # Crop to non-zero region\n",
    "    cropped = image[crop_index]\n",
    "    \n",
    "    # Step 2: Cut to min dimension of x, y to make square\n",
    "    crop_h, crop_w = cropped.shape[:2]\n",
    "    min_dim = min(crop_h, crop_w)\n",
    "    \n",
    "    # Calculate center crop\n",
    "    start_h = (crop_h - min_dim) // 2\n",
    "    start_w = (crop_w - min_dim) // 2\n",
    "    \n",
    "    square = cropped[start_h:start_h+min_dim, start_w:start_w+min_dim, :]\n",
    "    \n",
    "    # Calculate origin indices after square crop as slices\n",
    "    orig_min_row = min_indices[0] + start_h\n",
    "    orig_max_row = orig_min_row + min_dim\n",
    "    orig_min_col = min_indices[1] + start_w\n",
    "    orig_max_col = orig_min_col + min_dim\n",
    "    orig_min_z = min_indices[2]\n",
    "    orig_max_z = max_indices[2] + 1\n",
    "    \n",
    "    crop_index = (\n",
    "        slice(orig_min_row, orig_max_row),\n",
    "        slice(orig_min_col, orig_max_col),\n",
    "        slice(orig_min_z, orig_max_z)\n",
    "    )\n",
    "    \n",
    "    # Step 3: Resize to new_dim\n",
    "    current_dim = square.shape[0]\n",
    "    \n",
    "    if current_dim != new_dim:\n",
    "        resized = resize(square, (new_dim, new_dim, square.shape[2]), \n",
    "                       order=1,  # Linear interpolation\n",
    "                       anti_aliasing=True,\n",
    ")\n",
    "        # Ensure output dtype matches input\n",
    "        resized = resized.astype(square.dtype)\n",
    "    else:\n",
    "        resized = square.copy()\n",
    "    \n",
    "    # Step 4: Save crop_index for restoration\n",
    "    restore_info = {\n",
    "        'original_shape': image.shape,\n",
    "        'crop_index': crop_index,\n",
    "        'original_dim': current_dim,\n",
    "        'new_dim': new_dim\n",
    "    }\n",
    "    \n",
    "    return resized, restore_info\n",
    "\n",
    "def crop_resize_mask(mask, restore_info):\n",
    "    \"\"\"\n",
    "    Process a 3D numpy segmentation mask using restore_info from image processing,\n",
    "    cropping to the same square region and resizing to the same dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    mask (np.ndarray): Input segmentation mask of shape (x, y, z), same shape as original image\n",
    "    restore_info (dict): Restoration information from process_image, containing\n",
    "                        original_shape, crop_index, original_dim, new_dim\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (processed_mask, restore_info)\n",
    "        - processed_mask: Processed mask of shape (new_dim, new_dim, z)\n",
    "        - restore_info: Same restore_info for consistency in restoration\n",
    "    \"\"\"\n",
    "    # Validate mask shape\n",
    "    if mask.shape != restore_info['original_shape']:\n",
    "        raise ValueError(\"Mask shape must match original image shape\")\n",
    "    \n",
    "    # Step 1: Crop to the square region using crop_index\n",
    "    crop_index = restore_info['crop_index']\n",
    "    square = mask[crop_index]\n",
    "    \n",
    "    # Step 2: Resize to new_dim using skimage\n",
    "    current_dim = square.shape[0]\n",
    "    new_dim = restore_info['new_dim']\n",
    "    \n",
    "    if current_dim != new_dim:\n",
    "        resized = resize(square, output_shape=(new_dim, new_dim, square.shape[2]),\n",
    "                       order=0,\n",
    "                       anti_aliasing=False)\n",
    "        # Ensure output dtype matches input\n",
    "        resized = resized.astype(np.uint8)\n",
    "    else:\n",
    "        resized = square.copy()\n",
    "\n",
    "    return resized\n",
    "\n",
    "\n",
    "def restore_mask(processed_mask, restore_info):\n",
    "    \"\"\"\n",
    "    Restore a processed 3D segmentation mask back to its original shape.\n",
    "    \n",
    "    Improvements:\n",
    "    - Uses nearest-neighbor interpolation (order=0)\n",
    "    - Disables anti-aliasing (to preserve discrete labels)\n",
    "    - Ensures labels are integers after restoration\n",
    "    - Handles edge cases (padding, rounding) more robustly\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Resize back to the original square dimension (nearest-neighbor)\n",
    "    current_dim = processed_mask.shape[0]\n",
    "    original_dim = restore_info['original_dim']\n",
    "    \n",
    "    if current_dim != original_dim:\n",
    "        resized = resize(\n",
    "            processed_mask,\n",
    "            output_shape=(original_dim, original_dim, processed_mask.shape[2]),\n",
    "            order=0,               # nearest neighbor â†’ preserves labels\n",
    "            anti_aliasing=False,   # turn off to avoid soft edges\n",
    "            preserve_range=True    # keep label values as-is\n",
    "        )\n",
    "    else:\n",
    "        resized = processed_mask.copy()\n",
    "    \n",
    "    # Step 2: Initialize empty mask of the original shape\n",
    "    original_shape = restore_info['original_shape']\n",
    "    restored = np.zeros(original_shape, dtype=np.int16)\n",
    "    \n",
    "    # Step 3: Paste the restored square into its original crop position\n",
    "    crop_index = restore_info['crop_index']\n",
    "    restored[crop_index] = resized.astype(np.int16)  # round & ensure int labels\n",
    "\n",
    "    return restored\n",
    "\n",
    "\n",
    "\n",
    "def predict_patches(images, model, num_classes=4, batch_size=8, device=\"cuda\"):\n",
    "    \"\"\"return the patches\"\"\"\n",
    "    prediction = torch.zeros(\n",
    "        (images.size(0), num_classes, images.size(2), images.size(3)),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    while batch_start < images.size(0):\n",
    "        image = images[batch_start:batch_end]\n",
    "        with torch.no_grad():\n",
    "            image = image.to(device)\n",
    "            y_pred = model(image)\n",
    "            prediction[batch_start:batch_end] = y_pred\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size\n",
    "    return prediction.cpu().numpy()\n",
    "\n",
    "def predict_data_model(data, model):\n",
    "    probability_output = predict_patches(data[\"image\"], model) # shape (n, 5, 128, 128)\n",
    "    seg = np.argmax(probability_output, axis=1).transpose(1, 2, 0)  # shape (128, 128, n)\n",
    "    seg = remove_small_elements(seg, min_size_remove=300)\n",
    "    invert_seg = restore_mask(seg, data[\"restore_info\"])\n",
    "\n",
    "    return invert_seg\n",
    "\n",
    "def make_volume(ndarray, voxel_spacing):\n",
    "    volume = np.prod(voxel_spacing) * (ndarray.sum())\n",
    "    return volume\n",
    "\n",
    "def compute_dice(mask1, mask2, class_id, eps=1e-6):\n",
    "    mask1_binary = mask1==class_id\n",
    "    mask2_binary = mask2==class_id\n",
    "    intersection = np.sum(mask1_binary*mask2_binary)\n",
    "    union = np.sum(mask1_binary)+np.sum(mask2_binary)\n",
    "    return (2*intersection+eps)/(union+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_acdc = torch.load(\"tiramisu_acdc.pt\", weights_only=False)\n",
    "model_acdc.eval()\n",
    "model_acdc = model_acdc.to(device)\n",
    "\n",
    "def preprocess_data_table2(image_nrrd_path):\n",
    "    data = {}\n",
    "    patient_image = nrrd.read(image_nrrd_path)\n",
    "    image = patient_image[0]\n",
    "    image = min_max_normalize(image)\n",
    "\n",
    "    resized_image, restore_info = crop_resize_image(image, cfg.DATA.DIM_RESIZE)\n",
    "    # padded_mask = pad_background_with_index(mask, crop_index, padded_index, dim2pad=cfg.DATA.DIM2PAD)\n",
    "    data[\"restore_info\"] = restore_info\n",
    "    batch_images = []\n",
    "    for i in range(resized_image.shape[-1]):\n",
    "        slice_inputs = resized_image[..., i : i + 1]  # shape (224, 224, 1)\n",
    "        slices_image = torch.from_numpy(slice_inputs.transpose(-1, 0, 1))  # shape (1, 224, 224)\n",
    "        batch_images.append(slices_image)\n",
    "\n",
    "    batch_images = torch.stack(batch_images).float()  # shape (9,1, 224, 224)\n",
    "    data[\"image\"] = batch_images\n",
    "    return data\n",
    "    \n",
    "table2_path = \"Tables 2/Tables/*/\"\n",
    "list_test_image, list_test_mask = [], []\n",
    "for path in natsorted(glob.glob(table2_path)):\n",
    "    # go into the path and find nrrd files\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".nrrd\") and \"seg\" not in file:\n",
    "            list_test_image.append(path + file)\n",
    "        elif file.endswith(\"seg.nrrd\"):\n",
    "            list_test_mask.append(path + file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save the predicted masks\n",
    "os.makedirs(\"predicted_table2_data\", exist_ok=True)\n",
    "for index in range(len(list_test_image)):\n",
    "    image = nrrd.read(list_test_image[index])[0]\n",
    "    image_info = nrrd.read(list_test_image[index])[1]\n",
    "    data = preprocess_data_table2(list_test_image[index])\n",
    "\n",
    "    # seg = predict_data(data, segmenter, patient=patient, mvo=is_MVO, task=task).astype(np.uint8)\n",
    "    seg = predict_data_model(data, model_acdc).astype(np.uint8)\n",
    "    # give label 1, 3, 4 to 0\n",
    "    seg[seg == 1] = 0\n",
    "    seg[seg == 3] = 0\n",
    "    seg[seg == 4] = 0\n",
    "    # give label 2 to 1\n",
    "    seg[seg == 2] = 1\n",
    "    # write the predicted mask to the a nrrd file with image_info\n",
    "    # and make name of the file is like this 1 - PGF/34 de_high res PSIR EC_PSIR_tiramisu.nrrd\n",
    "    name_predict = list_test_image[index].split(\"/\")[-2:]\n",
    "    name_predict = \"/\".join(name_predict).replace(\".nrrd\", \"_tiramisu_seg.nrrd\")\n",
    "    # make folder dir 1-PGF to save the predicted mask\n",
    "    os.makedirs(f\"predicted_table2_data/{name_predict.split('/')[0]}\", exist_ok=True)\n",
    "\n",
    "    nrrd.write(f\"predicted_table2_data/{name_predict}\", seg, image_info)\n",
    "    print(\"write predicted mask to \", name_predict)\n",
    "    # break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
